<!DOCTYPE html>
<html lang="en-US">
<head>
    <meta charset="UTF-8">
    <title>2212.03551</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://cdn.mathpix.com/fonts/cmu.css"/>
    <style>
  html,body {
    width: 100%;
    height: 100%;
  }
  *, *::before,*::after {
    box-sizing: border-box;
  }
  @-ms-viewport {
    width: device-width;
  }
  body {
    margin: 0;
    color: #1E2029;
    font-size: 14px;
    line-height: normal;
  }
  hr {
    box-sizing: content-box;
    height: 0;
    overflow: visible;
  }
  h1, h2, h3, h4, h5, h6 {
    margin-top: 0;
    margin-bottom: 0.5em;
    color: rgba(0, 0, 0, 0.85);
    font-weight: 500;
  }
  p {
    margin-top: 0;
    margin-bottom: 1em;
  }
  ol, ul, dl {
    margin-top: 0;
    margin-bottom: 1em;
  }
  ol ol, ul ul, ol ul, ul ol {
    margin-bottom: 0;
  }
  dt {
    font-weight: 500;
  }
  dd {
    margin-bottom: 0.5em;
    margin-left: 0;
  }
  blockquote {
    margin: 0 0 1em;
  }
  dfn {
    font-style: italic;
  }
  b, strong {
    font-weight: bolder;
  }
  small {
    font-size: 80%;
  }
  sub, sup {
    position: relative;
    font-size: 75%;
    line-height: 0;
    vertical-align: baseline;
  }
  sub {
    bottom: -0.25em;
  }
  sup {
    top: -0.5em;
  }
  a {
    color: #0B93ff;
    text-decoration: none;
    background-color: transparent;
    outline: none;
    cursor: pointer;
    transition: color 0.3s;
  }
  a:hover {
    color: #33aaff;
  }
  a:active {
    color: #0070d9;
  }
  a:active, a:hover {
    text-decoration: none;
    outline: 0;
  }
  a[disabled] {
    color: rgba(0, 0, 0, 0.25)
    cursor: not-allowed;
    pointer-events: none;
  }
  pre, code, kbd, samp {
    font-size: 1em;
  }
  pre {
    margin-top: 0;
    margin-bottom: 1em;
    overflow: auto;
  }
  figure {
    margin: 0 0 1em;
  }
  img {
    vertical-align: middle;
    border-style: none;
  }
  svg:not(:root) {
    overflow: hidden;
  }
  table {
    border-collapse: collapse;
  }
  caption {
    padding-top: 0.75em;
    padding-bottom: 0.3em;
    color: rgba(0, 0, 0, 0.45)
    text-align: left;
    caption-side: bottom;
  }
  th {
    text-align: inherit;
  }

mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"] > svg a {
  fill: blue;
  stroke: blue;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

    #setText > div {
        justify-content: inherit;
        margin-top: 0;
        margin-bottom: 1em;
        
        
    }
    
    
    
    #setText div:last-child {
        margin-bottom: 0 !important;
    }

    #setText > br, #preview-content br {
        line-height: 1.2;
    }

    #preview-content > div {
        margin-top: 0;
        margin-bottom: 1em;
        
    }

    #preview-content table {
      margin-bottom: 1em;
    }

    #setText table {
      margin-bottom: 1em;
    }

    mjx-container {
      text-indent: 0;
      overflow-y: visible !important;
      padding-top: 1px;
      padding-bottom: 1px;
      
      
    }
    
    
    
    .math-inline mjx-container {
        display: inline-block !important;
        page-break-inside: avoid;
    }
    .math-block {
        align-items: center;
        min-width: min-content;
        page-break-after: auto;
        page-break-inside: avoid;
        margin-top: 1em;
        margin-bottom: 1em;
    }
    .math-block p {
        flex-shrink: 1;
    }
    .math-block mjx-container {
        margin: 0 !important;
    }
    .math-error {
        background-color: yellow;
        color: red;
    }

    #preview-content svg, #setText svg { min-width: initial !important;}

    #preview-content img, #setText img {
        max-width: 100%;
    }
    
    #preview-content blockquote,  #setText blockquote {
        page-break-inside: avoid;
        color: #666;
        margin: 0 0 1em 0;
        padding-left: 3em;
        border-left: .5em solid #eee;
    }

    #preview-content pre, #setText pre {
        border: 1px solid #ccc;
        page-break-inside: avoid;
        padding: 0.5em;
        background: #f8f8fa;
    }
    .empty {
        text-align: center;
        font-size: 18px;
        padding: 50px 0 !important;
    }

    #setText table, #preview-content table {
        display: table; 
        overflow: auto;
        max-width: 100%;
        border-collapse: collapse;
        page-break-inside: avoid;
    }
      
    #setText table th, #preview-content table th {
        text-align: center;
        font-weight: bold;
    }
    
    #setText table td, #preview-content table td,
    #setText table th, #preview-content table th {
        border: 1px solid #dfe2e5;
        padding: 6px 13px;
    }
      
    #setText table tr, #preview-content table tr {
        background-color: #fff;
        border-top: 1px solid #c6cbd1;
    }
    
    #setText table tr:nth-child(2n), #preview-content table tr:nth-child(2n) {
        background-color: #f6f8fa;
    }

    
    #setText .main-title, #setText .author, #preview-content .main-title, #preview-content .author  {
        text-align: center;
        margin: 0 auto;
    }
    
    #preview-content .main-title, #setText .main-title {
        line-height: 1.2;
        margin-bottom: 1em;
    }

    #preview-content .author, #setText .author  {
        display: flex;
        justify-content: center;
        flex-wrap: wrap;
    }

    #preview-content .author p, #setText .author p {
        min-width: 30%;
        max-width: 50%;
        padding: 0 7px;
    }

    #preview-content .author > p > span, #setText .author > p > span {
        display: block;
        text-align: center;
    }

    #preview-content .section-title, #setText .section-title {
        margin-top: 1.5em;
    }

    #preview-content .abstract, #setText .abstract {
        text-align: justify;
        margin-bottom: 1em;
    }

    #preview-content .abstract p, #setText .abstract p {
        margin-bottom: 0;
    }

    @media print {

      #preview {
        font-size: 10pt!important;
      }

      svg {
        shape-rendering: crispEdges;
      }

      .math-block svg, math-inline svg {
        margin-top: 1px;
      }

      #preview-content img, #setText img {
        display: block;
      }
      
      #preview-content .figure_img img, #setText .figure_img img {
        display: inline;
      }

      .preview-right {
        word-break: break-word;
      }

      #preview-content h1, #setText h1 {
        page-break-inside: avoid;
        position: relative;
        border: 2px solid transparent;
      }
  
      #preview-content h1::after, #setText h1::after {
        content: "";
        display: block;
        height: 100px;
        margin-bottom: -100px;
        position: relative;
      }
  
      #preview-content h2, #setText h2 {
        page-break-inside: avoid;
        position: relative;
        border: 2px solid transparent;
      }
  
      #preview-content h2::after, #setText h2::after {
        content: "";
        display: block;
        height: 100px;
        margin-bottom: -100px;
        position: relative;
      }
  
      #preview-content h3, #setText h3 {
        page-break-inside: avoid;
        position: relative;
        border: 2px solid transparent;
      }
  
      #preview-content h3::after, #setText h3::after {
        content: "";
        display: block;
        height: 100px;
        margin-bottom: -100px;
        position: relative;
      }
  
      #preview-content h4, #setText h4 {
        page-break-inside: avoid;
        position: relative;
        border: 2px solid transparent;
      }
  
      #preview-content h4::after, #setText h4::after {
        content: "";
        display: block;
        height: 100px;
        margin-bottom: -100px;
        position: relative;
      }
  
      #preview-content h5, #setText h5 {
        page-break-inside: avoid;
        position: relative;
        border: 2px solid transparent;
      }
  
      #preview-content h5::after, #setText h5::after {
        content: "";
        display: block;
        height: 100px;
        margin-bottom: -100px;
        position: relative;
      }
  
      #preview-content h6, #setText h6 {
        page-break-inside: avoid;
        position: relative;
        border: 2px solid transparent;
      }
  
      #preview-content h6::after, #setText h6::after {
        content: "";
        display: block;
        height: 100px;
        margin-bottom: -100px;
        position: relative;
      }
    }
    #preview-content sup, #setText sup {
      top: -.5em;
      position: relative;
      font-size: 75%;
      line-height: 0;
      vertical-align: baseline;
    }
    
    #preview-content .text-url, #setText .text-url {
      color: #0B93ff;
      cursor: text;
      pointer-events: none;
    }
    
    #preview-content .text-url a:hover, #setText .text-url a:hover {
      color: #0B93ff;
    }

    #preview-content code, #setText code {
      font-family: Inconsolata;
      font-size: inherit;
      display: initial;
      background: #f8f8fa;
    }
    #preview-content pre code, #setText pre code {
      font-size: inherit;
      display: block;
      color: #333;
      overflow-x: auto;
      font-size: 15px;
    }

    .hljs-comment,
    .hljs-quote {
      color: #998;
      font-style: italic;
    }

    .hljs-command {
      color: #005cc5;
    }

    .hljs-keyword,
    .hljs-selector-tag,
    .hljs-subst {
      color: #d73a49;
      font-weight: bold;
    }

    .hljs-number,
    .hljs-literal,
    .hljs-variable,
    .hljs-template-variable,
    .hljs-tag .hljs-attr {
      color: #005cc5;
    }

    .hljs-string,
    .hljs-doctag {
      color: #24292e;
    }

    .hljs-title,
    .hljs-section,
    .hljs-selector-id {
      color: #6f42c1;
      font-weight: bold;
    }

    .hljs-subst {
      font-weight: normal;
    }

    .hljs-type,
    .hljs-class .hljs-title {
      color: #458;
      font-weight: bold;
    }

    .hljs-tag,
    .hljs-name,
    .hljs-attribute {
      color: #000080;
      font-weight: normal;
    }

    .hljs-regexp,
    .hljs-link {
      color: #009926;
    }

    .hljs-symbol,
    .hljs-bullet {
      color: #990073;
    }

    .hljs-built_in,
    .hljs-builtin-name {
      color: #24292e;
    }

    .hljs-meta {
      color: #999;
      font-weight: bold;
    }

    .hljs-meta-keyword {
      color: #d73a49;
    }

    .hljs-meta-string {
      color: #032f62;
    }

    .hljs-deletion {
      background: #fdd;
    }

    .hljs-addition {
      background: #dfd;
    }

    .hljs-emphasis {
      font-style: italic;
    }

    .hljs-strong {
      font-weight: bold;
    }

    .table_tabular table th,  .table_tabular table th {
        border: none !important;
        padding: 6px 13px;
    }
      
    #tabular tr, #tabular tr {
        border-top: none !important;
        border-bottom: none !important;
    }
    #tabular td, #tabular td {
        border-style: none !important;
        background-color: #fff;
        border-color: #000 !important;
        word-break: keep-all;
        padding: 0.1em 0.5em !important;
    }
    #tabular {
        display: inline !important;
    }
    #tabular td > p {
        margin-bottom: 0;
        margin-top: 0;
    }
    #tabular td._empty {
      height: 1.3em;
    }
    #tabular td .f {
      opacity: 0;
    }
    
    html[data-theme="dark"] #tabular tr, html[data-theme="dark"] #tabular td {
      background-color: #202226;
      border-color: #fff !important;
    }  
    .table_tabular {
        overflow-x: auto;
        padding: 0 2px 0.5em 2px;
    }
    .figure_img {
       margin-bottom: 0.5em;
       overflow-x: auto;
    }

  ol.enumerate, ul.itemize {
    padding-inline-start: 40px;
  }
/* It's commented because counter not supporting to change value 
  ol.enumerate.lower-alpha {
    counter-reset: item ;
    list-style-type: none !important;
  }
  .enumerate.lower-alpha > li {
    position: relative;
  }
  .enumerate.lower-alpha > li:before { 
    content: "("counter(item, lower-alpha)")"; 
    counter-increment: item; 
    position: absolute;
    left: -47px;
    width: 47px;
    display: flex;
    justify-content: flex-end;
    padding-right: 7px;
    flex-wrap: nowrap;
    word-break: keep-all;
  }
  */
  
  .itemize > li {
    position: relative;
  }
  .itemize > li > span.li_level { 
    position: absolute;
    right: 100%;
    white-space: nowrap;
    width: max-content;;
    display: flex;
    justify-content: flex-end;
    padding-right: 10px;
    box-sizing: border-box;
  }

  #preview {
    font-family: 'CMU Serif', 'Georgia', Helvetica, Arial, sans-serif;
    font-size: 17px;
    visibility: visible;
    word-break: break-word;
    padding: 2.5em;
    max-width: 800px;
    margin: auto;
    box-sizing: content-box;
  }

  #preview h1, #preview h2, #preview h3, #preview h4, #preview h5, #preview strong {
    font-family: 'CMU Serif Bold', 'Georgia', Helvetica, Arial, sans-serif;
  }

  #preview  i, #preview  em {
    font-family: 'CMU Serif Italic', 'Georgia', Helvetica, Arial, sans-serif;
  }

  .mmd-menu {
    max-width: 320px;
    position: absolute;
    background-color: white;
    color: black;
    width: auto;
    padding: 5px 0px;
    border: 1px solid #E5E6EB;
    margin: 0;
    cursor: default;
    font: menu;
    text-align: left;
    text-indent: 0;
    text-transform: none;
    line-height: normal;
    letter-spacing: normal;
    word-spacing: normal;
    word-wrap: normal;
    white-space: nowrap;
    float: none;
    z-index: 201;
    border-radius: 5px;
    -webkit-border-radius: 5px;
    -moz-border-radius: 5px;
    -khtml-border-radius: 5px;
    box-shadow: 0px 10px 20px #808080;
    -webkit-box-shadow: 0px 10px 20px #808080;
    -moz-box-shadow: 0px 10px 20px #808080;
    -khtml-box-shadow: 0px 10px 20px #808080; 
  }
  
  .mmd-menu:focus { outline: none; }
  
  .mmd-menu.mmd-menu-sm {
    max-width: 100vw;
    padding-bottom: 34px;
    border-radius: 0;
    -webkit-border-radius: 0;
    -moz-border-radius: 0;
    -khtml-border-radius: 0;
  }

  .mmd-menu-item-icon {
    color: #1e2029;
    margin-left: auto;
    align-items: center;
    display: flex;
    flex-shrink: 0;
    display: none; 
  }

  .mmd-menu-item {
    padding-bottom: 8px;
    padding-top: 8px;
    padding-left: 1.25rem;
    padding-right: 1.25rem;
    display: flex;
    background: transparent; 
    height: 52px;
    max-height: 52px;
  }
  .mmd-menu-item:focus { outline: none; }

  .mmd-menu-item.active {
    background-color: #e1e0e5; 
  }

  .mmd-menu-item.active .mmd-menu-item-icon {
    display: flex; 
  }

  .mmd-menu-item-container {
    overflow: hidden; 
  }

  .mmd-menu-item-title {
    color: #1e2029;
    white-space: nowrap;
    text-overflow: ellipsis;
    overflow: hidden;
    font-size: 14px;
    line-height: 20px; 
  }

  .mmd-menu-item-value {
    color: #7d829c;
    white-space: nowrap;
    text-overflow: ellipsis;
    overflow: hidden;
    font-size: 12px;
    line-height: 16px; 
  }
  
  html[data-theme="dark"] .mmd-menu-item-title {
    color: #ebefe7;
  } 
  html[data-theme="dark"] .mmd-menu-item.active .mmd-menu-item-title {
    color: #1e2029;
  }
  html[data-theme="dark"] .mmd-menu {
    background-color: #33363a;
  }
  
  .mmd-context-menu-overlay{
    background: rgba(0, 0, 0, 0.56);
  }
  </style>
</head>
<body>
  <div id="preview" class="preview scrollEditor">
    <div id="container-ruller" />
    <div id="preview-content">
      <h1 type="title" class="main-title preview-paragraph-0 preview-line 0 1 2" id="talking-about-large-language-models" data_line_start="0" data_line_end="2" data_line="0,3" count_line="3">
Talking About Large Language Models</h1>
<div class="preview-paragraph-4 preview-line 4 5 6" data_line_start="4" data_line_end="6" data_line="4,7" count_line="3"><div class="author">
          <p><span>Murray Shanahan</span><span>Imperial College London</span><span>m.shanahan@imperial.ac.uk</span></p>
        </div></div>
<div class="preview-paragraph-8 preview-line 8" data_line_start="8" data_line_end="8" data_line="8,9" count_line="1">December 2022</div>
<div class="abstract preview-paragraph-10 preview-line 10 11 12" style="width: 80%; margin: 0 auto; margin-bottom: 1em; font-size: .9em;" data_line_start="10" data_line_end="12" data_line="10,13" count_line="3">
<h4 id="abstract_head" class="abstract_head" style="text-align: center;">
Abstract</h4>
<p style="text-indent: 1em;">Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as &quot;knows&quot;, &quot;believes&quot;, and &quot;thinks&quot;, when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.</p>
</div>
<h2 type="section" class="section-title preview-paragraph-14 preview-line 14" id="introduction" data_line_start="14" data_line_end="14" data_line="14,15" count_line="1">
<span class="section-number">1. </span>Introduction</h2>
<div class="preview-paragraph-16 preview-line 16" data_line_start="16" data_line_end="16" data_line="16,17" count_line="1">The advent of large language models (LLMs) such as Bert (Devlin et al., 2018) and GPT2 (Radford et al., 2019) was a game-changer for artificial intelligence. Based on transformer architectures (Vaswani et al., 2017), comprising hundreds of billions of parameters, and trained on hundreds of terabytes of textual data, their contemporary successors such as GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), and PaLM (Chowdhery et al., 2022) have given new meaning to the phrase &quot;unreasonable effectiveness of data&quot; (Halevy et al., 2009).</div>
<div class="preview-paragraph-18 preview-line 18" data_line_start="18" data_line_end="18" data_line="18,19" count_line="1">The effectiveness of these models is &quot;unreasonable&quot; (or, with the benefit of hindsight, somewhat surprising) in three inter-related ways. First, the performance of LLMs on benchmarks scales with the size of the training set (and, to a lesser degree with model size). Second, there are qualitative leaps in capability as the models scale. Third, a great many tasks that demand intelligence in humans can be reduced to next token prediction with a sufficiently performant model. It is the last of these three surprises that is the focus of the present paper.</div>
<div class="preview-paragraph-20 preview-line 20" data_line_start="20" data_line_end="20" data_line="20,21" count_line="1">As we build systems whose capabilities more and more resemble those of humans, despite the fact that those systems work in ways that are fundamentally different from the way humans work, it becomes increasingly tempting to anthropomorphise them. Humans have evolved to co-exist over many millions of years, and human culture has evolved over thousands of years to facilitate this co-existence, which ensures a degree of mutual understanding. But it is a serious mistake to unreflectingly apply to AI systems the same intuitions that we deploy in our dealings with each other, especially when those systems are so profoundly different from humans in their underlying operation.</div>
<div class="preview-paragraph-22 preview-line 22" data_line_start="22" data_line_end="22" data_line="22,23" count_line="1">The AI systems we are building today have considerable utility and enormous commercial potential, which imposes on us a great responsibility. To ensure that we can make informed decisions about the trustworthiness and safety of the AI systems we deploy, it is advisable to keep to the fore the way those systems actually work, and thereby to avoid imputing to them capacities they lack, while making the best use of the remarkable capabilities they genuinely possess.</div>
<h2 type="section" class="section-title preview-paragraph-24 preview-line 24" id="what-llms-really-do" data_line_start="24" data_line_end="24" data_line="24,25" count_line="1">
<span class="section-number">2. </span>What LLMs Really Do</h2>
<div class="preview-paragraph-26 preview-line 26" data_line_start="26" data_line_end="26" data_line="26,27" count_line="1">As Wittgenstein reminds us, human language use is an aspect of human collective behaviour, and it only makes sense in the wider context of the human social activity of which it forms a part (Wittgenstein, 1953). A human infant is born into a community of language users with which it shares a world, and it acquires language by interacting with this community and with the world it shares with them. As adults (or indeed as children past a certain age), when we have a casual conversation, we are engaging in an activity that is built upon this foundation. The same is true when we make a speech or send an email or deliver a lecture or write a paper. All of this language-involving activity makes sense because we inhabit a world we share with other language users.</div>
<div class="preview-paragraph-28 preview-line 28" data_line_start="28" data_line_end="28" data_line="28,29" count_line="1">A large language model is a very different sort of animal (Bender and Koller, 2020; Bender et al., 2021; Marcus and Davis, 2020). (Indeed, it is not an animal at all, which is very much to the point.) LLMs are generative mathematical models of the statistical distribution of tokens in the vast public corpus of humangenerated text, where the tokens in question include words, parts of words, or individual characters including punctuation marks. They are generative because we can sample from them, which means we can ask them questions. But the questions are of the following very specific kind. &quot;Here's a fragment of text. Tell me how this fragment might go on. According to your model of the statistics of human language, what words are likely to come next?&quot;'1</div>
<div class="preview-paragraph-30 preview-line 30" data_line_start="30" data_line_end="30" data_line="30,31" count_line="1">It is very important to bear in mind that this is what large language models really do. Suppose we give an LLM the prompt &quot;The first person to walk on the Moon was &quot;, and suppose it responds with &quot;Neil Armstrong&quot;. What are we really asking here? In an important sense, we are not really asking who was the first person to walk on the Moon. What we are really asking the model is the following question: Given the statistical distribution of words in the vast public corpus of (English) text, what words are most likely to follow the sequence &quot;The first person to walk on the Moon was &quot;? A good reply to this question is &quot;Neil Armstrong&quot;.</div>
<div class="preview-paragraph-32 preview-line 32" data_line_start="32" data_line_end="32" data_line="32,33" count_line="1">Similarly, we might give an LLM the prompt &quot;Twinkle twinkle &quot;, to which it will most likely respond &quot;little star&quot;. On one level, for sure, we are asking the model to remind us of the lyrics of a well-known nursery rhyme. But in an important sense what we are really doing is asking it the following question: Given the statis-</div>
<div class="preview-paragraph-34 preview-line 34" data_line_start="34" data_line_end="34" data_line="34,35" count_line="1"><span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>1</mn>
    </mrow>
  </msup>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>1</mn>
    </mrow>
  </msup>
</math></mathmlword><asciimath style="display: none;">^(1)</asciimath><latex style="display: none">{ }^{1}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="0.913ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 403.6 833.9" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"></g><g data-mml-node="TeXAtom" transform="translate(0, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow></mrow><mrow><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></span> Even if an LLM is fine-tuned, for example using reinforcement learning with human feedback (e.g. to filter out potentially toxic language) (Glaese et al., 2022), the result is still a model of the distribution of tokens in human language, albeit one that has been slightly perturbed. tical distribution of words in the public corpus, what words are most likely to follow the sequence &quot;Twinkle twinkle &quot;? To which an accurate answer is &quot;little star&quot;.</div>
<div class="preview-paragraph-36 preview-line 36" data_line_start="36" data_line_end="36" data_line="36,37" count_line="1">Here's a third example. Suppose you are the developer of an LLM and you prompt it with the words &quot;After the ring was destroyed, Frodo Baggins returned to &quot;, to which it responds &quot;the Shire&quot;. What are you doing here? On one level, it seems fair to say, you might be testing the model's knowledge of the fictional world of Tolkien's novels. But, in an important sense, the question you are really asking (as you presumably know, because you are the developer) is this: Given the statistical distribution of words in the public corpus, what words are most likely to follow the sequence &quot;After the ring was destroyed, Frodo Baggins returned to &quot;? To which an appropriate response is &quot;the Shire&quot;.</div>
<div class="preview-paragraph-38 preview-line 38" data_line_start="38" data_line_end="38" data_line="38,39" count_line="1">To the human user, each of these examples presents a different sort of relationship to truth. In the case of Neil Armstrong, the ultimate grounds for the truth or otherwise of the LLMs answer is the real world. The Moon is a real object and Neil Armstrong was a real person, and his walking on the Moon is a fact about the physical world. Frodo Baggins, on the other hand, is a fictional character, and the Shire is a fictional place. Frodo's return to the Shire is a fact about an imaginary world, not a real one. As for the little star in the nursery rhyme, well that is barely even a fictional object, and the only fact at issue is the occurrence of the words &quot;little star&quot; in a familiar English rhyme.</div>
<div class="preview-paragraph-40 preview-line 40" data_line_start="40" data_line_end="40" data_line="40,41" count_line="1">These distinctions are invisible at the level of what the LLM actually does, which is simply to generate statistically likely sequences of words. However, when we evaluate the utility of the model, these distinctions matter a great deal. There is no point in seeking Frodo's (fictional) descendants in the (real) English county of Surrey. This is one reason why it's a good idea for users to repeatedly remind themselves of what LLMs really do. It's also a good idea for developers to remind themselves of this, to avoid the misleading use of philosophically fraught words to describe the capabilities of LLMs, words such as &quot;belief&quot;, &quot;knowledge&quot;, &quot;understanding&quot;, &quot;self&quot;, or even &quot;consciousness&quot;.</div>
<h2 type="section" class="section-title preview-paragraph-42 preview-line 42" id="llms-and-the-intentional-stance" data_line_start="42" data_line_end="42" data_line="42,43" count_line="1">
<span class="section-number">3. </span>LLMs and the Intentional Stance</h2>
<div class="preview-paragraph-44 preview-line 44" data_line_start="44" data_line_end="44" data_line="44,45" count_line="1">It is perfectly natural to use anthropomorphic language in everyday conversations about artefacts, especially in the context of information technology. We do it all the time. My watch doesn't realise we're on daylight saving time. My phone thinks we're in the car park. The mail server won't talk to the network. And so on. These examples of what Dennett calls the intentional stance are harmless and useful forms of shorthand for complex processes whose details we don't know or care about. <span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>2</mn>
    </mrow>
  </msup>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>2</mn>
    </mrow>
  </msup>
</math></mathmlword><asciimath style="display: none;">^(2)</asciimath><latex style="display: none">{ }^{2}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="0.913ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 403.6 833.9" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"></g><g data-mml-node="TeXAtom" transform="translate(0, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow></mrow><mrow><mn>2</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></span> They are harmless because no-one takes them seriously enough to ask their watch to get it right next time, say, or to tell the mail server to try harder. Even without having read Dennett, everyone understands they are taking the intentional stance, that these are just useful turns of phrase.</div>
<div class="preview-paragraph-46 preview-line 46" data_line_start="46" data_line_end="46" data_line="46,47" count_line="1">The same consideration applies to LLMs, both for users and for developers. Insofar as everyone implicitly understands that these turns of phrase are just convenient shorthands, that they are taking the intentional stance, it does no harm to use them. However, in the case of LLMs, such is their power, things can get a little blurry. When an LLM can be made to improve its performance on reasoning tasks simply by being told to &quot;think step by step&quot; (Koiima et al., 2022) (to pick just one remarkable discovery), the temptation to see it as having human-like characteristics is almost overwhelming.</div>
<div class="preview-paragraph-48 preview-line 48" data_line_start="48" data_line_end="48" data_line="48,49" count_line="1">To be clear, it is not the argument of this paper that a system based on a large language model could never, in principle, warrant description in terms of beliefs, intentions, reason, etc. Nor does the paper advocate any particular account of belief, of intention, or of any other philosophically contentious concept 3 Rather, the point is that such systems are simultaneously so very different from humans in their construction, yet (often but not always) so human-like in their behaviour, that we need to pay careful attention to how they work before we speak of them in language suggestive of human capabilities and patterns of behaviour.</div>
<div class="preview-paragraph-50 preview-line 50" data_line_start="50" data_line_end="50" data_line="50,51" count_line="1">To sharpen the issue, let's compare two very</div>
<div class="preview-paragraph-52 preview-line 52" data_line_start="52" data_line_end="52" data_line="52,53" count_line="1">2 &quot;The intentional stance is the strategy of interpreting the behavior of an entity <span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo>&#x2026;</mo>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo>…</mo>
</math></mathmlword><asciimath style="display: none;">dots</asciimath><latex style="display: none">\ldots</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.652ex" height="0.271ex" role="img" focusable="false" viewBox="0 -120 1172 120" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>…</mo></math></mjx-assistive-mml></mjx-container></span> by treating it as if it were a rational agent&quot; (Dennett, 2009).</div>
<div class="preview-paragraph-54 preview-line 54" data_line_start="54" data_line_end="54" data_line="54,55" count_line="1"><span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>3</mn>
    </mrow>
  </msup>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>3</mn>
    </mrow>
  </msup>
</math></mathmlword><asciimath style="display: none;">^(3)</asciimath><latex style="display: none">{ }^{3}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="0.913ex" height="1.885ex" role="img" focusable="false" viewBox="0 -833.2 403.6 833.2" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"></g><g data-mml-node="TeXAtom" transform="translate(0, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow></mrow><mrow><mn>3</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></span> In particular, when I use the term &quot;really&quot;, as in the question 'Does X &quot;really&quot; have Y?', I am not assuming there is some metaphysical fact of the matter here. Rather, the question is whether, when more is revealed about the nature of X, we still want to use the word Y. short conversations, one between Alice and Bob (both human), and a second between Alice and BOT, a fictional question-answering system based on a large language model. Suppose Alice asks Bob &quot;What country is to the south of Rwanda?&quot; and Bob replies &quot;I think it's Burundi&quot;. Shortly afterwards, because Bob is often wrong in such matters, Alice presents the same question to BOT, which (to her mild disappointment) offers the same answer: &quot;Burundi is to the south of Rwanda&quot;. Alice might now reasonably remark that both Bob and BOT knew that Burundi was south of Rwanda. But what is really going on here? Is the word &quot;know&quot; being used in the same sense in the two cases?</div>
<h2 type="section" class="section-title preview-paragraph-56 preview-line 56" id="humans-and-llms-compared" data_line_start="56" data_line_end="56" data_line="56,57" count_line="1">
<span class="section-number">4. </span>Humans and LLMs Compared</h2>
<div class="preview-paragraph-58 preview-line 58" data_line_start="58" data_line_end="58" data_line="58,59" count_line="1">What is Bob, a representative human, doing when he correctly answers a straightforward factual question in an everyday conversation? To begin with, Bob understands that the question comes from another person (Alice), that his answer will be heard by that person, and that it will have an effect on what she believes. In fact, after many years together, Bob knows a good deal else about Alice that is relevant to such situations: her background knowledge, her interests, her opinion of him, and so on. All of this frames the communicative intent behind his reply, which is to impart a certain fact to her, given his understanding of what she wants to know.</div>
<div class="preview-paragraph-60 preview-line 60" data_line_start="60" data_line_end="60" data_line="60,61" count_line="1">Moreover, when Bob announces that Burundi is to the south of Rwanda, he is doing so against the backdrop of various human capacities that we all take for granted when we engage in everyday commerce with each other. There is a whole battery of techniques we can call upon to ascertain whether a sentence expresses a true proposition, depending on what sort of sentence it is. We can investigate the world directly, with our own eyes and ears. We can consult Google or Wikipedia, or even a book. We can ask someone who is knowledgeable on the relevant subject matter. We can try to think things through, rationally, by ourselves, but we can also argue things out with our peers. All of this relies on there being agreed criteria external to ourselves against which what we say can be assessed.</div>
<div class="preview-paragraph-62 preview-line 62" data_line_start="62" data_line_end="62" data_line="62,63" count_line="1">How about BOT? What is going on when a large language model is used to answer such questions? First, it's worth noting that a bare-bones LLM is, by itself, not a conversational agent 4 For a start, the LLM will have to be embedded in a larger system to manage the turn-taking in the dialogue. But it will also need to be coaxed into producing conversation-like behaviour 5 Recall that an LLM simply generates sequences of words that are statistically likely follow-ons from a given prompt. But the sequence &quot;What country is to the south of Rwanda? Burundi is to the south of Rwanda&quot;, with both sentences squashed together exactly like that, may not, in fact, be very likely. A more likely pattern, given that numerous plays and film scripts feature in the public corpus, would be something like the following.</div>
<div class="preview-paragraph-64 preview-line 64" data_line_start="64" data_line_end="64" data_line="64,65" count_line="1">Fred: What country is south of Rwanda? Jane: Burundi is south of Rwanda.</div>
<div class="preview-paragraph-66 preview-line 66" data_line_start="66" data_line_end="66" data_line="66,67" count_line="1">Of course, those exact words may not appear, but their likelihood, in the statistical sense, will be high. In short, BOT will be much better at generating appropriate responses if they conform to this pattern rather than to the pattern of actual human conversation. Fortunately, the user (Alice) doesn't have to know anything about this. In the background, the LLM is invisibly prompted with a prefix along the following lines. This is a conversation between User, a human, and BOT, a clever and knowledgeable AI agent.</div>
<div class="preview-paragraph-68 preview-line 68" data_line_start="68" data_line_end="68" data_line="68,69" count_line="1">User: What is 2+2?</div>
<div class="preview-paragraph-70 preview-line 70" data_line_start="70" data_line_end="70" data_line="70,71" count_line="1">BOT: The answer is 4.</div>
<div class="preview-paragraph-72 preview-line 72" data_line_start="72" data_line_end="72" data_line="72,73" count_line="1">User: Where was Albert Einstein born? BOT: He was born in Germany.</div>
<div class="preview-paragraph-74 preview-line 74" data_line_start="74" data_line_end="74" data_line="74,75" count_line="1">Alice's query, in the following form, is appended to this prefix.</div>
<div class="preview-paragraph-76 preview-line 76" data_line_start="76" data_line_end="76" data_line="76,77" count_line="1">User: What country is south of Rwanda? BOT :</div>
<div class="preview-paragraph-78 preview-line 78" data_line_start="78" data_line_end="78" data_line="78,79" count_line="1">This yields the full prompt to be submitted to the LLM, which will hopefully predict a continuation along the lines we are looking for, i.e. &quot;Burundi is south of Rwanda&quot;.</div>
<div class="preview-paragraph-80 preview-line 80" data_line_start="80" data_line_end="80" data_line="80,81" count_line="1">Dialogue is just one application of LLMs that can be facilitated by the judicious use of prompt prefixes. In a similar way, LLMs can be adapted to perform numerous tasks without further training (Brown et al., 2020). This has led to a whole new category of AI research, namely prompt engineering, which will remain relevant until we have better models of the relationship between</div>
<div class="preview-paragraph-82 preview-line 82" data_line_start="82" data_line_end="82" data_line="82,83" count_line="1"><span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>4</mn>
    </mrow>
  </msup>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>4</mn>
    </mrow>
  </msup>
</math></mathmlword><asciimath style="display: none;">^(4)</asciimath><latex style="display: none">{ }^{4}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="0.913ex" height="1.904ex" role="img" focusable="false" viewBox="0 -841.7 403.6 841.7" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"></g><g data-mml-node="TeXAtom" transform="translate(0, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow></mrow><mrow><mn>4</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></span> Strictly speaking, the large language model itself comprises just the model architecture and the trained parameters.</div>
<div class="preview-paragraph-84 preview-line 84" data_line_start="84" data_line_end="84" data_line="84,85" count_line="1"><span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>5</mn>
    </mrow>
  </msup>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>5</mn>
    </mrow>
  </msup>
</math></mathmlword><asciimath style="display: none;">^(5)</asciimath><latex style="display: none">{ }^{5}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="0.913ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 403.6 833.9" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"></g><g data-mml-node="TeXAtom" transform="translate(0, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></g></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow></mrow><mrow><mn>5</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></span> See Thoppilan et al. (2022) for an example of such a system, as well as a useful survey of related dialogue work. what we say and what we want.</div>
<h2 type="section" class="section-title preview-paragraph-86 preview-line 86" id="do-llms-really-know-anything%3F" data_line_start="86" data_line_end="86" data_line="86,87" count_line="1">
<span class="section-number">5. </span>Do LLMs Really Know Anything?</h2>
<div class="preview-paragraph-88 preview-line 88" data_line_start="88" data_line_end="88" data_line="88,89" count_line="1">Turning an LLM into a question-answering system by a) embedding it in a larger system, and b) using prompt engineering to elicit the required behaviour exemplifies a pattern found in much contemporary work. In a similar fashion, LLMs can be used not only for question-answering, but also to summarise news articles, to generate screenplays, to solve logic puzzles, and to translate between languages, among other things. There are two important takeaways here. First, the basic function of a large language model, namely to generate statistically likely continuations of word sequences, is extraordinarily versatile. Second, notwithstanding this versatility, at the heart of every such application is a model doing just that one thing: generating statistically likely continuations of word sequences.</div>
<div class="preview-paragraph-90 preview-line 90" data_line_start="90" data_line_end="90" data_line="90,91" count_line="1">With this insight to the fore, let's revisit the question of how LLMs compare to humans, and reconsider the propriety of the language we use to talk about them. In contrast to humans like Bob and Alice, a simple LLM-based questionanswering system, such as BOT, has no communicative intent (Bender and Koller, 2020). In no meaningful sense, even under the licence of the intentional stance, does it know that the questions it is asked come from a person, or that a person is on the receiving end of its answers. By implication, it knows nothing about that person. It has no understanding of what they want to know nor of the effect its response will have on their beliefs.</div>
<div class="preview-paragraph-92 preview-line 92" data_line_start="92" data_line_end="92" data_line="92,93" count_line="1">Moreover, in contrast to its human interlocutors, a simple LLM-based question-answering system like BOT does not properly speaking have beliefs 6 BOT does not &quot;really&quot; know that Burundi is south of Rwanda, although the intentional stance does, in this case, license Alice's casual remark to the contrary. To see this, we need to think separately about the underlying LLM and the system in which it is embedded. First, let's consider the underlying LLM, that is to say the bare-bones model, comprising the</div>
<div class="preview-paragraph-94 preview-line 94" data_line_start="94" data_line_end="94" data_line="94,95" count_line="1"><span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>6</mn>
    </mrow>
  </msup>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>6</mn>
    </mrow>
  </msup>
</math></mathmlword><asciimath style="display: none;">^(6)</asciimath><latex style="display: none">{ }^{6}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="0.913ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 403.6 833.9" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"></g><g data-mml-node="TeXAtom" transform="translate(0, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path></g></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow></mrow><mrow><mn>6</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></span> This paper focuses on belief, knowledge, and reason. Others have argued about meaning in LLMs (Bender and Koller, 2020; Piantadosi and Hill, 2022). Here we take no particular stand on meaning, instead preferring questions about how words are used, whether they are words generated by the LLMs themselves or words generated by humans that are about LLMs. model architecture and the trained parameters.</div>
<div class="preview-paragraph-96 preview-line 96" data_line_start="96" data_line_end="96" data_line="96,97" count_line="1">A bare-bones LLM doesn't &quot;really&quot; know anything because all it does, at a fundamental level, is sequence prediction. Sometimes a predicted sequence takes the form of a proposition. But the special relationship propositional sequences have to truth is apparent only to the humans who are asking questions, or to those who provided the data the model was trained on. Sequences of words with a propositional form are not special to the model itself in the way they are to us. The model itself has no notion of truth or falsehood, properly speaking, because it lacks the means to exercise these concepts in anything like the way we do.</div>
<div class="preview-paragraph-98 preview-line 98" data_line_start="98" data_line_end="98" data_line="98,99" count_line="1">It could perhaps be argued that an LLM &quot;knows&quot; what words typically follow what other words, in a sense that does not rely on the intentional stance. But even if we allow this, knowing that the word &quot;Burundi&quot; is likely to succeed the words &quot;The country to the south of Rwanda is&quot; is not the same as knowing that Burundi is to the south of Rwanda. To confuse those two things is to make a profound category mistake. If you doubt this, consider whether knowing that the word &quot;little&quot; is likely to follow the words &quot;Twinkle, twinkle&quot; is the same as knowing that twinkle twinkle little. The idea doesn't even make sense.</div>
<div class="preview-paragraph-100 preview-line 100" data_line_start="100" data_line_end="100" data_line="100,101" count_line="1">So much for the bare-bones language model. What about the whole dialogue system of which the LLM is the core component? Does that have beliefs, properly speaking? At least the very idea of the whole system having beliefs makes sense. There is no category error here. However, for a simple dialogue agent like BOT, the answer is surely still &quot;no&quot;. A simple LLM-based questionanswering system like BOT lacks the means to use the words &quot;true&quot; and &quot;false&quot; in all the ways, and in all the contexts, that we do. It cannot participate fully in the human language game of truth, because it does not inhabit the world we human language-users share <span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mn>4</mn>
    <mrow>
      <mn>7</mn>
    </mrow>
  </msup>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mn>4</mn>
    <mrow>
      <mn>7</mn>
    </mrow>
  </msup>
</math></mathmlword><asciimath style="display: none;">4^(7)</asciimath><latex style="display: none">4^{7}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.044ex" height="1.903ex" role="img" focusable="false" viewBox="0 -841 903.6 841" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="TeXAtom" transform="translate(500, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path></g></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mn>4</mn><mrow><mn>7</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></span></div>
<h2 type="section" class="section-title preview-paragraph-102 preview-line 102" id="what-about-emergence%3F" data_line_start="102" data_line_end="102" data_line="102,103" count_line="1">
<span class="section-number">6. </span>What About Emergence?</h2>
<div class="preview-paragraph-104 preview-line 104" data_line_start="104" data_line_end="104" data_line="104,105" count_line="1">Contemporary large language models are so powerful, so versatile, and so useful that the argument above might be difficult to accept. Exchanges with state-of-the-art (in late 2022) LLMbased conversational agents are so convincing, it is hard to not to anthropomorphise them. Could</div>
<div class="preview-paragraph-106 preview-line 106" data_line_start="106" data_line_end="106" data_line="106,107" count_line="1"><span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>7</mn>
    </mrow>
  </msup>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>7</mn>
    </mrow>
  </msup>
</math></mathmlword><asciimath style="display: none;">^(7)</asciimath><latex style="display: none">{ }^{7}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="0.913ex" height="1.903ex" role="img" focusable="false" viewBox="0 -841 403.6 841" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"></g><g data-mml-node="TeXAtom" transform="translate(0, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path></g></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow></mrow><mrow><mn>7</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></span> For a discussion of the &quot;language game of truth&quot;, see Shanahan (2010), pp.36-39. it be that something more complex and subtle is going on here?</div>
<div class="preview-paragraph-108 preview-line 108" data_line_start="108" data_line_end="108" data_line="108,109" count_line="1">One tempting line of argument goes like this. Although large language models, at root, only perform sequence prediction, it's possible that, in learning to do this, they have discovered emergent mechanisms that warrant a description in higher-level terms. These higher-level terms might include &quot;knowledge&quot; and &quot;belief&quot;. Indeed, we know that artificial neural networks can approximate any computable function to an arbitrary degree of accuracy. So whatever mechanisms are needed to enable the formation of beliefs, they probably reside in the parameter space somewhere. Given a big enough model, enough data of the right sort, and enough computing power to train the model, perhaps stochastic gradient descent can discover such mechanisms if they are the best way to optimise the objective of making accurate sequence predictions.</div>
<div class="preview-paragraph-110 preview-line 110" data_line_start="110" data_line_end="110" data_line="110,111" count_line="1">This argument has considerable appeal. After all, the overriding lesson of recent progress in LLMs is that extraordinary and unexpected capabilities emerge when big enough models are trained on very large quantities of textual data. However, as long as our considerations are confined to a simple LLM-based question-answering system, this has little bearing on the issue of communicative intent. It doesn't matter what internal mechanisms it uses, a sequence predictor is not, in itself, the kind of thing that could, even in principle, have communicative intent, and simply embedding it in a dialogue management system will not help.</div>
<div class="preview-paragraph-112 preview-line 112" data_line_start="112" data_line_end="112" data_line="112,113" count_line="1">But what about knowledge and belief? Could a sophisticated emergent mechanism of the right sort license us to speak of an LLM as if it &quot;really&quot; knew or believed something? Again, it's important to distinguish between the bare-bones model and the whole system. Only in the context of a capacity to distinguish truth from falsehood can we legitimately speak of &quot;belief&quot; in its fullest sense. But an LLM is not in the business of making judgements. It just models what words are likely to follow from what other words. The internal mechanisms it uses to do this, whatever they are, cannot in themselves be sensitive to the truth or otherwise of the word sequences it predicts.</div>
<div class="preview-paragraph-114 preview-line 114" data_line_start="114" data_line_end="114" data_line="114,115" count_line="1">Of course, it is perfectly acceptable to say that an LLM &quot;encodes&quot;, &quot;stores&quot;, or &quot;contains&quot; knowledge, in the same sense that an encyclopedia can be said to encode, store, or contain knowledge. Indeed, it can reasonably be claimed that one emergent property of an LLM is that it encodes kinds of knowledge of the everyday world and the way it works that no encyclopedia captures (Li et al., 2021). But if Alice were to remark that &quot;Wikipedia knew that Burundi was south of Rwanda&quot;, it would be a figure of speech, not a literal statement. An encyclopedia doesn't literally &quot;know&quot; or &quot;believe&quot; anything, in the way that a human does, and neither does a bare-bones LLM.</div>
<div class="preview-paragraph-116 preview-line 116" data_line_start="116" data_line_end="116" data_line="116,117" count_line="1">The real issue here is that, whatever emergent properties it has, the LLM itself has no access to any external reality against which its words might be measured, nor the means to apply any other external criteria of truth, such as agreement with other language-users 8 It only makes sense to speak of such criteria in the context of the system as a whole, and for a system as a whole to meet them, it needs to be more than a simple conversational agent. In the words of B.C.Smith, it must &quot;authentically engage with the world's being the way in which [its] representations represent it as being&quot; (Smith, 2019).</div>
<h2 type="section" class="section-title preview-paragraph-118 preview-line 118" id="%247-%5Cquad%24-external-information-sources" data_line_start="118" data_line_end="118" data_line="118,119" count_line="1">
<span class="section-number">7. </span><span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mn>7</mn>
  <mstyle scriptlevel="0">
    <mspace width="1em"></mspace>
  </mstyle>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mn>7</mn>
  <mstyle scriptlevel="0">
    <mspace width="1em"></mspace>
  </mstyle>
</math></mathmlword><asciimath style="display: none;">7quad</asciimath><latex style="display: none">7 \quad</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="3.394ex" height="1.579ex" role="img" focusable="false" viewBox="0 -676 1500 698" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path></g><g data-mml-node="mstyle" transform="translate(500, 0)"><g data-mml-node="mspace"></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>7</mn><mstyle scriptlevel="0"><mspace width="1em"></mspace></mstyle></math></mjx-assistive-mml></mjx-container></span> External Information Sources</h2>
<div class="preview-paragraph-120 preview-line 120" data_line_start="120" data_line_end="120" data_line="120,121" count_line="1">The point here does not concern any specific belief. It concerns the prerequisites for ascribing any beliefs at all to a system. Nothing can count as a belief about the world we share - in the largest sense of the term - unless it is against the backdrop of the ability to update beliefs appropriately in the light of evidence from that world, an essential aspect of the capacity to distinguish truth from falsehood.</div>
<div class="preview-paragraph-122 preview-line 122" data_line_start="122" data_line_end="122" data_line="122,123" count_line="1">Could Wikipedia, or some other trustworthy factual website, provide external criteria against which the truth or falsehood of a belief might be measured? Suppose an LLM were embedded in a system that regularly consulted such sources (as LaMDA does (Thoppilan et al., 2022)), and used a contemporary model editing technique to maintain the factual accuracy of its predictions (such as the one described by Meng et al. <span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo stretchy="false">(</mo>
  <mn>2022</mn>
  <mo stretchy="false">)</mo>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo stretchy="false">(</mo>
  <mn>2022</mn>
  <mo stretchy="false">)</mo>
</math></mathmlword><asciimath style="display: none;">(2022)</asciimath><latex style="display: none">(2022)</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="6.285ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2778 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(389, 0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500, 0)"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(1000, 0)"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(1500, 0)"></path></g><g data-mml-node="mo" transform="translate(2389, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mn>2022</mn><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></span> ). Would this not count as exercising the</div>
<div class="preview-paragraph-124 preview-line 124" data_line_start="124" data_line_end="124" data_line="124,125" count_line="1"><span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>8</mn>
    </mrow>
  </msup>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>8</mn>
    </mrow>
  </msup>
</math></mathmlword><asciimath style="display: none;">^(8)</asciimath><latex style="display: none">{ }^{8}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="0.913ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 403.6 833.9" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"></g><g data-mml-node="TeXAtom" transform="translate(0, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path></g></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow></mrow><mrow><mn>8</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></span> Davidson uses a similar argument to call into question whether belief is possible without language (Davidson, 1982). The point here is different. We are concerned with conditions that have to be met for the generation of a natural language sentence to reflect the possession of a propositional attitude.</div>
<div class="preview-paragraph-126 preview-line 126" data_line_start="126" data_line_end="126" data_line="126,127" count_line="1"><span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>9</mn>
    </mrow>
  </msup>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>9</mn>
    </mrow>
  </msup>
</math></mathmlword><asciimath style="display: none;">^(9)</asciimath><latex style="display: none">{ }^{9}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="0.913ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 403.6 833.9" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"></g><g data-mml-node="TeXAtom" transform="translate(0, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path></g></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow></mrow><mrow><mn>9</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></span> Commendably, Meng et al. (2022) use the term &quot;factual associations&quot; to denote the information that under- required sort of capacity to update belief in the light of evidence?</div>
<div class="preview-paragraph-128 preview-line 128" data_line_start="128" data_line_end="128" data_line="128,129" count_line="1">Crucially, this line of thinking depends on the shift from the language model itself to the larger system of which the language model is a part. The language model itself is still just a sequence predictor, and has no more access to the external world than it ever did. It is only with respect to the whole system that the intentional stance becomes more compelling in such a case. But before yielding to it, we should remind ourselves of how very different such systems are from human beings. When Alice took to Wikipedia and confirmed that Burundi was south of Rwanda, what took place was more than just an update to a model in her head of the distribution of word sequences in the English language.</div>
<div class="preview-paragraph-130 preview-line 130" data_line_start="130" data_line_end="130" data_line="130,131" count_line="1">The change that took place in Alice was a reflection of her nature as a language-using animal inhabiting a shared world with a community of other language-users. Humans are the natural home of talk of beliefs and the like, and the behavioural expectations that go hand-in-hand with such talk are grounded in our mutual understanding, which is itself the product of a common evolutionary heritage. When we interact with an AI system based on a large language model, these grounds are absent, an important consideration when deciding whether or not to speak of such a system as if it &quot;really&quot; had beliefs.</div>
<h2 type="section" class="section-title preview-paragraph-132 preview-line 132" id="vision-language-models" data_line_start="132" data_line_end="132" data_line="132,133" count_line="1">
<span class="section-number">8. </span>Vision-Language Models</h2>
<div class="preview-paragraph-134 preview-line 134" data_line_start="134" data_line_end="134" data_line="134,135" count_line="1">A sequence predictor may not by itself be the kind of thing that could have communicative intent or form beliefs about an external reality. But, as repeatedly emphasised, LLMs in the wild must be embedded in larger architectures to be useful. To build a question-answering system, the LLM simply has to be supplemented with a dialogue management system that queries the model as appropriate. There is nothing this larger architecture does that might count as communicative intent or the capacity to form beliefs. So the point stands.</div>
<div class="preview-paragraph-136 preview-line 136" data_line_start="136" data_line_end="136" data_line="136,137" count_line="1">However, LLMs can be combined with other sorts of models and / or embedded in more complex architectures. Vision-language models (VLMs) such as VilBERT (Lu et al., 2019) and Flamingo (Alavrac et al., 2022), for example, combine a language model with an image</div>
<div class="preview-paragraph-138 preview-line 138" data_line_start="138" data_line_end="138" data_line="138,139" count_line="1">lies an LLM's ability to generate word sequences with a propositional form. encoder, and are trained on a multi-modal corpus of text-image pairs. This enables them to predict how a given sequence of words will continue in the context of a given image. VLMs can be used for visual question-answering or to engage in a dialogue about a user-provided image.</div>
<div class="preview-paragraph-140 preview-line 140" data_line_start="140" data_line_end="140" data_line="140,141" count_line="1">Could a user-provided image stand in for an external reality against which the truth or falsehood of a proposition can be assessed? Could it be legitimate to speak of a VLM's beliefs, in the full sense of the term? We can indeed imagine a VLM that uses an LLM to generate hypotheses about an image, then verifies their truth with respect to that image (perhaps by consulting a human), and then fine-tunes the LLM not to make statements that turn out to be false. Talk of belief here would perhaps be less problematic.</div>
<div class="preview-paragraph-142 preview-line 142" data_line_start="142" data_line_end="142" data_line="142,143" count_line="1">However, most contemporary VLM-based systems don't work this way. Rather, they depend on frozen models of the joint distribution of text and images. In this respect, the relationship between a user-provided image and the words generated by the VLM is fundamentally different from the relationship between the world shared by humans and the words we use when we talk about that world. Importantly, the former relationship is mere correlation, while the latter is causal 10</div>
<div class="preview-paragraph-144 preview-line 144" data_line_start="144" data_line_end="144" data_line="144,145" count_line="1">The consequences of the lack of causality are troubling. If the user presents the VLM with a picture of a dog, and the VLM says &quot;This is a picture of a dog&quot;, there is no guarantee that its words are connected with the dog in particular, rather than some other feature of the image that is spuriously correlated with dogs (such as the presence of a kennel). Conversely, if the VLM says there is a dog in an image, there is no guarantee that there actually is a dog, rather than just a kennel.</div>
<div class="preview-paragraph-146 preview-line 146" data_line_start="146" data_line_end="146" data_line="146,147" count_line="1">Whether or not these concerns apply to any specific VLM-based system depends on exactly how that system works; what sort of model it uses, and how that model is embedded in the system's overall architecture. But to the extent that the relationship between words and things for a given VLM-based system is different than it is for human language-users, it might be prudent not to take literally talk of what that system &quot;knows&quot; or &quot;believes&quot;.</div>
<div class="preview-paragraph-148 preview-line 148" data_line_start="148" data_line_end="148" data_line="148,149" count_line="1"><span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>10</mn>
    </mrow>
  </msup>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>10</mn>
    </mrow>
  </msup>
</math></mathmlword><asciimath style="display: none;">^(10)</asciimath><latex style="display: none">{ }^{10}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.713ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 757.1 833.9" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"></g><g data-mml-node="TeXAtom" transform="translate(0, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500, 0)"></path></g></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow></mrow><mrow><mn>10</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></span> Of course, there is causal structure to the computations carried out by the model during inference. But this is not the same as there being causal relations between words and the things those words are taken to be about.</div>
<h2 type="section" class="section-title preview-paragraph-150 preview-line 150" id="what-about-embodiment%3F" data_line_start="150" data_line_end="150" data_line="150,151" count_line="1">
<span class="section-number">9. </span>What About Embodiment?</h2>
<div class="preview-paragraph-152 preview-line 152" data_line_start="152" data_line_end="152" data_line="152,153" count_line="1">Humans are members of a community of language-users inhabiting a shared world, and this primal fact makes them essentially different to large language models. Human language users can consult the world to settle their disagreements and update their beliefs. They can, so to speak, &quot;triangulate&quot; on objective reality. In isolation, an LLM is not the sort of thing that can do this, but in application, LLMs are embedded in larger systems. What if an LLM is embedded in a system capable of interacting with a world external to itself? What if the system in question is embodied, either physically in a robot or virtually in an avatar?</div>
<div class="preview-paragraph-154 preview-line 154" data_line_start="154" data_line_end="154" data_line="154,155" count_line="1">When such a system inhabits a world like our own - a world populated with 3D objects, some of which are other agents, some of whom are language-users - it is, in this important respect, a lot more human-like than a disembodied language model. But whether or not it is appropriate to speak of communicative intent in the context of such a system, or of knowledge and belief, in their fullest sense, depends on exactly how the LLM is embodied.</div>
<div class="preview-paragraph-156 preview-line 156" data_line_start="156" data_line_end="156" data_line="156,157" count_line="1">As an example, let's consider the SayCan system of Ahn et al. (2022). In this work, an LLM is embedded in a system that controls a physical robot. The robot carries out everyday tasks (such as clearing a spillage) in accordance with a user's high-level natural language instruction. The job of LLM is to map the user's instruction to low-level actions (such as finding a sponge) that will help the robot to achieve the required goal. This is done via an engineered prompt prefix that makes the model output natural language descriptions of suitable low-level actions, scoring them for usefulness.</div>
<div class="preview-paragraph-158 preview-line 158" data_line_start="158" data_line_end="158" data_line="158,159" count_line="1">The language model component of the SayCan system suggests actions without taking into account what the environment actually affords the robot at the time. Perhaps there is a sponge to hand. Perhaps not. Accordingly, a separate perceptual module assesses the scene using the robot's sensors and determines the current feasibility of performing each low-level action. Combining the LLM's estimate of each action's usefulness with the perceptual module's estimate of each action's feasibility yields the best action to attempt next.</div>
<div class="preview-paragraph-160 preview-line 160" data_line_start="160" data_line_end="160" data_line="160,161" count_line="1">SayCan exemplifies the many innovative ways that a large language model can be put to use. Moreover, it could be argued that the natu- ral language descriptions of recommended lowlevel actions generated by the LLM are grounded thanks to their role as intermediaries between perception and action 11 Nevertheless, despite being physically embodied and interacting with the real world, the way language is learned and used in a system like SayCan is very different from the way it is learned and used by a human.</div>
<div class="preview-paragraph-162 preview-line 162" data_line_start="162" data_line_end="162" data_line="162,163" count_line="1">The language models incorporated in systems like SayCan are pre-trained to perform sequence prediction in a disembodied setting from a textonly dataset. They have not learned language by talking to other language-users while immersed in a shared world and engaged in joint activity. SayCan is suggestive of the kind of embodied language-using system we might see in the future. But in such systems today, the role of language is very limited. The user issues instructions to the system in natural language, and the system generates interpretable natural language descriptions of its actions. But this tiny repertoire of language use hardly bears comparison to the cornucopia of collective activity that language supports in humans.</div>
<div class="preview-paragraph-164 preview-line 164" data_line_start="164" data_line_end="164" data_line="164,165" count_line="1">The upshot of this is that we should be just as cautious in our choice of words when talking about embodied systems incorporating LLMs as we are when talking about disembodied systems that incorporate LLMs. Under the licence of the intentional stance, a user might say that a robot knew there was a cup to hand if it stated &quot;I can get you a cup&quot; and proceeded to do so. But if pressed, the wise engineer might demur when asked whether the robot really understood the situation, especially if its repertoire is confined to a handful of simple actions in a carefully controlled environment.</div>
<h2 type="section" class="section-title preview-paragraph-166 preview-line 166" id="can-language-models-reason%3F" data_line_start="166" data_line_end="166" data_line="166,167" count_line="1">
<span class="section-number">10. </span>Can Language Models Reason?</h2>
<div class="preview-paragraph-168 preview-line 168" data_line_start="168" data_line_end="168" data_line="168,169" count_line="1">While the answer to the question &quot;Do LLMbased systems really have beliefs?&quot; is usually &quot;no&quot;, the question &quot;Can LLM-based systems really reason?&quot; is harder to settle. This is because reasoning, insofar as it is founded in formal logic, is content neutral. The modus ponens rule of inference, for example, is valid whatever the premises are about. If all squirgles are splonky and Gilfred is a squirgle then it follows that Gil-</div>
<div class="preview-paragraph-170 preview-line 170" data_line_start="170" data_line_end="170" data_line="170,171" count_line="1"><span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>11</mn>
    </mrow>
  </msup>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>11</mn>
    </mrow>
  </msup>
</math></mathmlword><asciimath style="display: none;">^(11)</asciimath><latex style="display: none">{ }^{11}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.713ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 757.1 833.9" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"></g><g data-mml-node="TeXAtom" transform="translate(0, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" transform="translate(500, 0)"></path></g></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow></mrow><mrow><mn>11</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></span> None of the symbols manipulated by an LLM are grounded in the sense of Harnad (1990), that is to say through perception, except indirectly and parasitically through the humans who generated the original training data. fred is splonky. The conclusion follows from the premises here irrespective of the meaning (if any) of &quot;squirgle&quot; and &quot;splonky&quot;, and whoever the unfortunate Gilfred might be.</div>
<div class="preview-paragraph-172 preview-line 172" data_line_start="172" data_line_end="172" data_line="172,173" count_line="1">The content neutrality of logic means that we cannot criticise talk of reasoning in LLMs on the grounds that they have no access to an external reality against which truth or falsehood can be measured. However, as always, it's crucial to keep in mind what LLMs really do. If we prompt an LLM with &quot;All humans are mortal and Socrates is human therefore&quot;, we are not instructing it to carry out deductive inference. Rather, we are asking it the following question. Given the statistical distribution of words in the public corpus, what words are likely to follow the sequence 'All humans are mortal and Socrates is human therefore&quot;.</div>
<div class="preview-paragraph-174 preview-line 174" data_line_start="174" data_line_end="174" data_line="174,175" count_line="1">If reasoning problems could be solved with nothing more than a single step of deductive inference, then an LLM's ability to answer questions such as this might be sufficient. But nontrivial reasoning problems require multiple inference steps. LLMs can be effectively applied to multi-step reasoning, without further training, thanks to clever prompt engineering. In chain-ofthought prompting, for example, a prompt prefix is submitted to the model, before the user's query, containing a few examples of multi-step reasoning, with all the intermediate steps explicitly spelled out (Nye et al., 2021; Wei et al., 2022).</div>
<div class="preview-paragraph-176 preview-line 176" data_line_start="176" data_line_end="176" data_line="176,177" count_line="1">Including a prompt prefix in the chain-ofthought style encourages the model to generate follow-on sequences in the same style, which is to say comprising a series of explicit reasoning steps that lead to the final answer. As usual, the question really being posed to the model is of the form &quot;Given the statistical distribution of words in the public corpus, what words are likely to follow the sequence S&quot;, where in this case the sequence <span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>S</mi>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>S</mi>
</math></mathmlword><asciimath style="display: none;">S</asciimath><latex style="display: none">S</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.459ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 645 727" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="53" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>S</mi></math></mjx-assistive-mml></mjx-container></span> is the chain-of-thought prompt prefix plus the user's query. The sequences of tokens that are most likely to follow <span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow>
    <mi mathvariant="normal">S</mi>
  </mrow>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow>
    <mi mathvariant="normal">S</mi>
  </mrow>
</math></mathmlword><asciimath style="display: none;">S</asciimath><latex style="display: none">\mathrm{S}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.258ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 556 727" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="53" d="M55 507Q55 590 112 647T243 704H257Q342 704 405 641L426 672Q431 679 436 687T446 700L449 704Q450 704 453 704T459 705H463Q466 705 472 699V462L466 456H448Q437 456 435 459T430 479Q413 605 329 646Q292 662 254 662Q201 662 168 626T135 542Q135 508 152 480T200 435Q210 431 286 412T370 389Q427 367 463 314T500 191Q500 110 448 45T301 -21Q245 -21 201 -4T140 27L122 41Q118 36 107 21T87 -7T78 -21Q76 -22 68 -22H64Q61 -22 55 -16V101Q55 220 56 222Q58 227 76 227H89Q95 221 95 214Q95 182 105 151T139 90T205 42T305 24Q352 24 386 62T420 155Q420 198 398 233T340 281Q284 295 266 300Q261 301 239 306T206 314T174 325T141 343T112 367T85 402Q55 451 55 507Z"></path></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="normal">S</mi></mrow></math></mjx-assistive-mml></mjx-container></span> will have a similar form to sequences found in the prompt prefix, which is to say they will include multiple steps of reasoning, so these are what the model generates.</div>
<div class="preview-paragraph-178 preview-line 178" data_line_start="178" data_line_end="178" data_line="178,179" count_line="1">Remarkably, not only do the model's responses take the form of an argument with multiple steps, the argument in question is often (but not always) valid, and the final answer is often (but not always) correct. To the extent that a suitably prompted LLM appears to reason correctly, it does so by mimicking well-formed arguments in its training set and / or in the prompt. But could this mimicry ever amount to genuine reasoning? Even if today's models make occasional mistakes, could further scaling iron these out to the point that a model's performance was indistinguishable from that of a hard-coded reasoning algorithm, such as we find in a theorem prover, for example? Maybe. But how would we know? How could we come to trust such a model?</div>
<div class="preview-paragraph-180 preview-line 180" data_line_start="180" data_line_end="180" data_line="180,181" count_line="1">Well, the sequences of sentences generated by a theorem prover are faithful to logic, in the sense that they are the result of an underlying computational process whose causal structure mirrors the inferential structure of the problem (Creswell and Shanahan, 2022). One way to build a trustworthy reasoning system using an LLM is to embed it in an algorithm that enforces the same causal structure (Creswell and Shanahan, 2022; Creswell et al., 2022). But if we stuck with a pure LLM, the only way to fully trust the arguments it generated would be by reverse engineering it and discovering emergent mechanisms that conformed to the faithful reasoning prescription. In the mean time, we should proceed with caution, and use discretion when characterising what these models do.</div>
<h2 type="section" class="section-title preview-paragraph-182 preview-line 182" id="conclusion%3A-why-this-matters" data_line_start="182" data_line_end="182" data_line="182,183" count_line="1">
<span class="section-number">11. </span>Conclusion: Why This Matters</h2>
<div class="preview-paragraph-184 preview-line 184" data_line_start="184" data_line_end="184" data_line="184,185" count_line="1">Does the foregoing discussion amount to anything more than philosophical nitpicking? Surely when researchers talk of &quot;beliefs&quot;, &quot;knowledge&quot;, &quot;reason&quot;, and the like, the meaning of those terms is perfectly clear. In papers, researchers use such terms as a convenient shorthand for precisely defined computational mechanisms, as allowed by the intentional stance. Well, this is fine as long as there is no possibility of anyone assigning more weight to such terms than they can legitimately bear, if there is no danger of their use misleading anyone about the character and capabilities of the systems being described.</div>
<div class="preview-paragraph-186 preview-line 186" data_line_start="186" data_line_end="186" data_line="186,187" count_line="1">However, today's large language models, and the applications that use them, are so powerful, so convincingly intelligent, that such licence can no longer safely be applied (Ruane et al., 2019; Weidinger et al., 2021). As AI practitioners, the way we talk about LLMs matters. It matters not only when we write scientific papers, but also when we interact with policy makers or speak to the media. The careless use of philosophically loaded words like &quot;believes&quot; and &quot;thinks&quot; is especially problematic, because such terms obfuscate mechanism and actively encourage anthropomorphism.</div>
<div class="preview-paragraph-188 preview-line 188" data_line_start="188" data_line_end="188" data_line="188,189" count_line="1">Interacting with a contemporary LLM-based conversational agent can create a compelling illusion of being in the presence of a thinking creature like ourselves. Yet in their very nature, such systems are fundamentally not like ourselves. The shared &quot;form of life&quot; that underlies mutual understanding and trust among humans is absent, and these systems can be inscrutable as a result, presenting a patchwork of less-than-human with superhuman capacities, of uncannily human-like with peculiarly inhuman behaviours.</div>
<div class="preview-paragraph-190 preview-line 190" data_line_start="190" data_line_end="190" data_line="190,191" count_line="1">The sudden presence among us of exotic, mind-like entities might precipitate a shift in the way we use familiar psychological terms like &quot;believes&quot; and &quot;thinks&quot;, or perhaps the introduction of new words and turns of phrase. But it takes time for new language to settle, and for new ways of talking to find their place in human affairs. It may require an extensive period of interacting with, of living with, these new kinds of artefact before we learn how best to talk about them. 12 Meanwhile, we should try to resist the siren call of anthropomorphism.</div>
<h2 type="section" class="section-title preview-paragraph-192 preview-line 192" id="acknowledgments" data_line_start="192" data_line_end="192" data_line="192,193" count_line="1">
<span class="section-number">12. </span>Acknowledgments</h2>
<div class="preview-paragraph-194 preview-line 194" data_line_start="194" data_line_end="194" data_line="194,195" count_line="1">Thanks to Toni Creswell, Richard Evans, Christos Kaplanis, Andrew Lampinen, and Kyriacos Nikiforou for invaluable (and robust) discussions on the topic of this paper.</div>
<h2 type="section" class="section-title preview-paragraph-196 preview-line 196" id="references" data_line_start="196" data_line_end="196" data_line="196,197" count_line="1">
<span class="section-number">13. </span>References</h2>
<div class="preview-paragraph-198 preview-line 198" data_line_start="198" data_line_end="198" data_line="198,199" count_line="1">M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, et al. Do as I can, not as I say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.</div>
<div class="preview-paragraph-200 preview-line 200" data_line_start="200" data_line_end="200" data_line="200,201" count_line="1">J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.</div>
<div class="preview-paragraph-202 preview-line 202" data_line_start="202" data_line_end="202" data_line="202,203" count_line="1">E. Bender and A. Koller. Climbing towards NLU: On meaning, form, and understanding in the</div>
<div class="preview-paragraph-204 preview-line 204" data_line_start="204" data_line_end="204" data_line="204,205" count_line="1"><span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>12</mn>
    </mrow>
  </msup>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow></mrow>
    <mrow>
      <mn>12</mn>
    </mrow>
  </msup>
</math></mathmlword><asciimath style="display: none;">^(12)</asciimath><latex style="display: none">{ }^{12}</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.713ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 757.1 833.9" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"></g><g data-mml-node="TeXAtom" transform="translate(0, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500, 0)"></path></g></g></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow></mrow><mrow><mn>12</mn></mrow></msup></math></mjx-assistive-mml></mjx-container></span> Ideally, we would also like a theoretical understanding of their inner workings. But at present, despite some commendable work in the right direction (Elhage et al., 2021; Li et al., 2021; Olsson et al., 2022), we still await such an understanding. age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185-5198, 2020.</div>
<div class="preview-paragraph-206 preview-line 206" data_line_start="206" data_line_end="206" data_line="206,207" count_line="1">E. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610-623, 2021.</div>
<div class="preview-paragraph-208 preview-line 208" data_line_start="208" data_line_end="208" data_line="208,209" count_line="1">T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901, 2020.</div>
<div class="preview-paragraph-210 preview-line 210" data_line_start="210" data_line_end="210" data_line="210,211" count_line="1">A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arxiv:2204.02311, 2022.</div>
<div class="preview-paragraph-212 preview-line 212" data_line_start="212" data_line_end="212" data_line="212,213" count_line="1">A. Creswell and M. Shanahan. Faithful reasoning using large language models. arXiv preprint arXiv:2208.14271, 2022.</div>
<div class="preview-paragraph-214 preview-line 214" data_line_start="214" data_line_end="214" data_line="214,215" count_line="1">A. Creswell, M. Shanahan, and I. Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.</div>
<div class="preview-paragraph-216 preview-line 216" data_line_start="216" data_line_end="216" data_line="216,217" count_line="1">D. Davidson. Rational animals. Dialectica, 36: 317-327, 1982.</div>
<div class="preview-paragraph-218 preview-line 218" data_line_start="218" data_line_end="218" data_line="218,219" count_line="1">D. Dennett. Intentional systems theory. In The Oxford Handbook of Philosophy of Mind, pages 339-350. Oxford University Press, 2009.</div>
<div class="preview-paragraph-220 preview-line 220" data_line_start="220" data_line_end="220" data_line="220,221" count_line="1">J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</div>
<div class="preview-paragraph-222 preview-line 222" data_line_start="222" data_line_end="222" data_line="222,223" count_line="1">N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. <a href="https://transformercircuits.pub/2021/framework/index.html" target="_blank" rel="noopener" style="word-break: break-all">https://transformercircuits.pub/2021/framework/index.html</a>.</div>
<div class="preview-paragraph-224 preview-line 224" data_line_start="224" data_line_end="224" data_line="224,225" count_line="1">A. Glaese, N. McAleese, M. Trȩbacz, J. Aslanides, V. Firoiu, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022. A. Y. Halevy, P. Norvig, and F. Pereira. The unreasonable effectiveness of data. IEEE Intelligent Systems, 24(2):8-12, 2009.</div>
<div class="preview-paragraph-226 preview-line 226" data_line_start="226" data_line_end="226" data_line="226,227" count_line="1">S. Harnad. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1-3): 335-346, 1990.</div>
<div class="preview-paragraph-228 preview-line 228" data_line_start="228" data_line_end="228" data_line="228,229" count_line="1">T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</div>
<div class="preview-paragraph-230 preview-line 230" data_line_start="230" data_line_end="230" data_line="230,231" count_line="1">B. Z. Li, M. Nye, and J. Andreas. Implicit representations of meaning in neural language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021.</div>
<div class="preview-paragraph-232 preview-line 232" data_line_start="232" data_line_end="232" data_line="232,233" count_line="1">J. Lu, D. Batra, D. Parikh, and S. Lee. ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019.</div>
<div class="preview-paragraph-234 preview-line 234" data_line_start="234" data_line_end="234" data_line="234,235" count_line="1">G. Marcus and E. Davis. GPT-3, bloviator: OpenAI's language generator has no idea what it's talking about. MIT Technology Review, August 2020, 2020.</div>
<div class="preview-paragraph-236 preview-line 236" data_line_start="236" data_line_end="236" data_line="236,237" count_line="1">K. Meng, D. Bau, A. J. Andonian, and Y. Belinkov. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems, 2022.</div>
<div class="preview-paragraph-238 preview-line 238" data_line_start="238" data_line_end="238" data_line="238,239" count_line="1">M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</div>
<div class="preview-paragraph-240 preview-line 240" data_line_start="240" data_line_end="240" data_line="240,241" count_line="1">C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, et al. In-context learning and induction heads. Transformer Circuits Thread, 2022. <a href="https://transformercircuits.pub/2022/in-context-learning-andinduction-heads/index.html" target="_blank" rel="noopener" style="word-break: break-all">https://transformercircuits.pub/2022/in-context-learning-andinduction-heads/index.html</a>.</div>
<div class="preview-paragraph-242 preview-line 242" data_line_start="242" data_line_end="242" data_line="242,243" count_line="1">S. T. Piantadosi and F. Hill. Meaning without reference in large language models. arXiv preprint arXiv:2208.02957, 2022.</div>
<div class="preview-paragraph-244 preview-line 244" data_line_start="244" data_line_end="244" data_line="244,245" count_line="1">A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. 2019. J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, et al. Scaling language models: Methods, analysis &amp; insights from training Gopher. arXiv preprint arXiv:2112.11446, 2021.</div>
<div class="preview-paragraph-246 preview-line 246" data_line_start="246" data_line_end="246" data_line="246,247" count_line="1">E. Ruane, A. Birhane, and A. Ventresque. Conversational AI: Social and ethical considerations. In Proceedings 27th AIAI Irish Conference on Artificial Intelligence and Cognitive Science, pages 104-115, 2019.</div>
<div class="preview-paragraph-248 preview-line 248" data_line_start="248" data_line_end="248" data_line="248,249" count_line="1">M. Shanahan. Embodiment and the Inner Life: Cognition and Consciousness in the Space of Possible Minds. Oxford University Press, 2010.</div>
<div class="preview-paragraph-250 preview-line 250" data_line_start="250" data_line_end="250" data_line="250,251" count_line="1">B. C. Smith. The Promise of Artificial Intelligence: Reckoning and Judgment. MIT Press, 2019.</div>
<div class="preview-paragraph-252 preview-line 252" data_line_start="252" data_line_end="252" data_line="252,253" count_line="1">R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, et al. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.</div>
<div class="preview-paragraph-254 preview-line 254" data_line_start="254" data_line_end="254" data_line="254,255" count_line="1">A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, E. Kaiser, and I. Polosukhin. Attention is all you need. In <span class="math-inline "><mathml style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>A</mi>
  <mi>d</mi>
  <mo>&#x2212;</mo>
</math></mathml><mathmlword style="display: none"><math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>A</mi>
  <mi>d</mi>
  <mo>−</mo>
</math></mathmlword><asciimath style="display: none;">Ad-</asciimath><latex style="display: none">A d-</latex><mjx-container class="MathJax" jax="SVG" role="presentation" style="position: relative"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="4.633ex" height="1.805ex" role="img" focusable="false" viewBox="0 -716 2048 798" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="41" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(750, 0)"><path data-c="64" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(1270, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g></g></g></svg><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mi>d</mi><mo>−</mo></math></mjx-assistive-mml></mjx-container></span> vances in Neural Information Processing Systems, pages 5998-6008, 2017.</div>
<div class="preview-paragraph-256 preview-line 256" data_line_start="256" data_line_end="256" data_line="256,257" count_line="1">J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022.</div>
<div class="preview-paragraph-258 preview-line 258" data_line_start="258" data_line_end="258" data_line="258,259" count_line="1">L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359, 2021.</div>
<div class="preview-paragraph-260 preview-line 260" data_line_start="260" data_line_end="260" data_line="260,261" count_line="1">L. Wittgenstein. Philosophical Investigations. (Translated by Anscombe, G.E.M.). Basil Blackwell, 1953.</div>

    </div>
  </div>
</body>
</html>
